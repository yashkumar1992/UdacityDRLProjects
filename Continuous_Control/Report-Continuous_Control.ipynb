{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous_Control - Project Report\n",
    "\n",
    "## Learning Algorithm\n",
    "DDPG - Deep Deterministic Policy Gradient is an Model-Free Off-Policy Algorithm for Learning Conitnuous Actions.\n",
    "It is based on Actor- Criti Architecture , where Actor predicts an Action and Critic predicts whehter that Action is good based on the action and current state.\n",
    "Apart from DQN, DDPG differs by having 2 separate Netwroks for Target.\n",
    "DDPG uses Ornstein-Uhlenbeck process to introduce Noise for better generalisation (like in Representation Learning Scenarios)\n",
    "Critic Network minimises the MSE(Mean Squared Error) from the expected return while the Actor network tries maximise predictions that get acceptance from critic network.\n",
    "\n",
    "    \n",
    "### DDPG -  Neural Network Architecture\n",
    "#### Actor\n",
    "Actor has 3 hidden layers with the initial layer being applied Batch Normalization. This is done to normalize various low-dimentional features of different continuous values.\n",
    "\n",
    "#### Critic\n",
    "Critic has 3 hidden layers with Drop Out Regularisation in the final layer\n",
    "\n",
    "\n",
    "### DDPG -  Agent\n",
    "We use the above mentioned architecture to train the Agent in Learning a Task by sending reward signals.\n",
    "Agent has following Functionalities\n",
    "##### Act:\n",
    "This the Agent's Policy to choose an Action. We can use Softmax, Epsilon Greedy for this. We are using Epsilon Greedy.  Epsilon - Greedy - works by randomizing exploration n% of the times thereby learning from those explorations.\n",
    "I have tried out the Decaying Learning Rate in Noise, which boosted the performance.\n",
    "##### Step:\n",
    "Agent(s) accumulate their experience by storing the states,action,reward information into a windowed data structure.\n",
    "Then data points are sampled from this database and sent to training. This is known as Experience Replay and this is done to remove the sequential correlation between actions and re-learning from the rare experiences.\n",
    "Agent learns 10 times every 20 timesteps to avoid frequent update at every time step.\n",
    "##### Learn:\n",
    "The actor network updates the weights of policy parameters and predicts action and the Critic Network generates the value for the predicted action and given state.\n",
    "#### Parameters:\n",
    "\n",
    "Hidden Layers - 300,200\n",
    "\n",
    "BUFFER_SIZE = int(1e6)  - replay buffer size  \n",
    "BATCH_SIZE = 512        - minibatch size  \n",
    "GAMMA = 0.99            - discount factor  \n",
    "TAU = 1e-3              - for soft update of target parameters  \n",
    "LR_ACTOR = 1e-4         - learning rate of the actor   \n",
    "LR_CRITIC = 1e-3        - learning rate of the critic  \n",
    "WEIGHT_DECAY = 0        - L2 weight decay  \n",
    "N_LEARN_UPDATES = 10    - number of learning updates to do once step is called  \n",
    "N_TIME_STEPS = 20       - learn only at every N time steps  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Reward Plot\n",
    "Please find the avg reward for every 100 episodes in the attached file(same folder). 30+ is the cut-off mean score across all agents\n",
    "\n",
    "\n",
    "##### The agent solved the Environement in 61 episodes with Average Score of 30.09\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Ideas for Future Work\n",
    "Please find the Future Ideas in the following :  \n",
    "1. Train with Different Architecture Extensions to DQN like PPO,A2C etc\n",
    "2. Analyze the Impact of Decayed Noise Term\n",
    "3. Analyze the Impact of larger time step vs smaller one.\n",
    "4. Analyze the Impact of different values of drop out regularization\n",
    "5. Introduce Prioritized Experience Replay to upweight most important experiences\n",
    "6. To Introduce Own Implementation of Experience Replay governed by the environment\n",
    "7. Train a deeper Network and understand its learning capability of complex features\n",
    "8. Explore Representation Learning in High Dimensional State Space Scenarios\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
